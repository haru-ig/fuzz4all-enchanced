# This file includes the configuration parameters for the fuzzing approach

# --------------------------------------------------------------------------- #
# Configuration of the overall fuzzing setup
fuzzing:
  # where to store the results (relative to the where you are calling fuzz.py)
  # Updated folder name to reflect the new model
  output_folder: ../fastd/fuzzall/FuzzAll/experiment/full_run/cpp/run_deepseek
  # number of fuzzing iterations
  num: 10000
  # total fuzzing time in hours
  total_time: 24
  # level of logging: 1 = INFO, 2 = TRACE, 3 = VERBOSE
  log_level: 3
  # use to validate fuzzing outputs on the fly.
  # If flag not set then only generation will be done. (Default: false)
  otf: true
  # use to resume a previous fuzzing run.
  # If flag not set then a new fuzzing run will be started (Default: false)
  resume: true
  # use to evaluate the fuzzing results.
  # If flag not set then no evaluation will be done (Default: false)
  evaluate: false
  # use hand-written prompt to query the LLM model
  use_hand_written_prompt: false
  # whether to use only trigger_to_generate_input and input_hint, without the
  # any documentation information or example code
  no_input_prompt: false
  # prompt strategy to generate obtain programs after the first one.
  # 0: generate new code using separator
  # 1: mutate existing code
  # 2: semantically equivalent code generation
  # 3: combine previous two code generations
  prompt_strategy: 2


# --------------------------------------------------------------------------- #
# Configuration of the target system
target:
  # language to fuzz, currently supported: cpp, smt2, java, go
  language: c
  # path to documentation of the feature of the target system
  # (Relative to the root of the fuzzing framework)
  path_documentation: config/documentation/c/c_goto.md
  # path to the example code using the feature of the target system
  # (Relative to the root of the fuzzing framework)
  path_example_code:
  # path to the command to push the llm to generate the input for the
  # target system using the given feature
  trigger_to_generate_input: "/* Please create a short program which combines goto with new C features in a complex way */"
  # hint to give to the llm to generate the input for the target system
  input_hint: "#include <stdlib.h>"
  # path to the hand-written prompt to give to the llm
  # (Relative to the root of the fuzzing framework)
  path_hand_written_prompt:
  # string to check if the generated input is valid. If the string is present
  # in the generated input, the input is considered valid.
  target_string: "goto"


# --------------------------------------------------------------------------- #
# Configuration of the Large Language Model (LLM) setup
llm:
  # temperature to query the LLM model when generating output.
  # 1.0 provides good diversity for fuzzing.
  temperature: 1
  # batch size
  # INCREASED: DeepSeek-7B is half the size of StarCoderBase.
  # You can likely double the batch size (e.g., 30 -> 60) to speed up fuzzing.
  # If you get CUDA OOM errors, revert to 30.
  batch_size: 60
  # use hardware acceleration (GPU) for the LLM model
  device: cuda
  # model name according to the HuggingFace model hub
  # MODIFIED: Points to the DeepSeek Base model
  model_name: deepseek-ai/deepseek-coder-7b-base-v1.5
  # local model folder (absolute path)
  # if not set, the model will be downloaded from the HuggingFace model hub
  # model_folder: /home/username/local_model_repository
  # additional end of sequence tokens
  # DeepSeek specific EOS tokens if needed, though standard ones usually suffice.
  # additional_eos_tokens:
  #   - "<|EOT|>"
  # maximum length of the generated llm output
  # INCREASED: DeepSeek supports 16K context. 2048 allows for richer, more complex test cases.
  max_length: 2048